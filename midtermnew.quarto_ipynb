{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Midterm NYC Aansh Jha\"\n",
        "format:\n",
        "  pdf:\n",
        "    code-fold: false\n",
        "jupyter: python3\n",
        "---\n",
        "\n",
        "\n",
        "# 1. Data Cleaning"
      ],
      "id": "bdd472a1"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import pyarrow.feather as feather\n",
        "from uszipcode import SearchEngine\n",
        "\n",
        "# Load the dataset\n",
        "noise_311_data = pd.read_csv('data/nypd311w063024noise_by100724.csv')\n",
        "print(noise_311_data.info())\n",
        "\n",
        "# Standardize column names\n",
        "noise_311_data.columns = noise_311_data.columns.str.lower().str.replace(' ', '_')\n",
        "print(noise_311_data.columns)\n",
        "\n",
        "# Count missing values in each column\n",
        "print(noise_311_data.isnull().sum())\n",
        "\n",
        "# Display total observations\n",
        "print(f\"Total observations: {noise_311_data.shape[0]}\")\n",
        "\n",
        "# Remove unnecessary columns\n",
        "noise_311_data = noise_311_data.drop('taxi_company_borough', axis=1)\n",
        "noise_311_data = noise_311_data.drop('taxi_pick_up_location', axis=1)\n",
        "noise_311_data = noise_311_data.drop('bridge_highway_name', axis=1)\n",
        "noise_311_data = noise_311_data.drop('bridge_highway_direction', axis=1)\n",
        "noise_311_data = noise_311_data.drop('road_ramp', axis=1)\n",
        "noise_311_data = noise_311_data.drop('bridge_highway_segment', axis=1)\n",
        "noise_311_data = noise_311_data.drop('facility_type', axis=1)\n",
        "noise_311_data = noise_311_data.drop('due_date', axis=1)\n",
        "\n",
        "# Check for duplicate records\n",
        "duplicate_count = noise_311_data.duplicated().sum()\n",
        "print(f\"Number of duplicate records: {duplicate_count}\")\n",
        "\n",
        "# Identify columns with a single unique value\n",
        "unique_value_cols = []\n",
        "\n",
        "for col in noise_311_data.columns:\n",
        "    unique_count = noise_311_data[col].nunique()\n",
        "    if unique_count == 1:\n",
        "        unique_value_cols.append(col)\n",
        "\n",
        "print(f\"Columns with a single unique value: {unique_value_cols}\")\n",
        "\n",
        "\n",
        "# Drop columns with all missing values\n",
        "noise_311_data = noise_311_data.dropna(axis=1, how='all')\n",
        "print(noise_311_data.columns)\n",
        "\n",
        "# Compare 'agency' and 'agency_name' columns\n",
        "print(noise_311_data[['agency', 'agency_name']].head())\n",
        "\n",
        "csv_file_path = os.path.join('data', 'nypd311w063024noise_by100724.csv')\n",
        "\n",
        "csv_size_mb = os.path.getsize(csv_file_path) / 1000000\n",
        "print(f\"CSV file size: {csv_size_mb:.2f} MB\")\n",
        "\n",
        "feather.write_feather(noise_311_data, 'data/noise_311_cleaned.feather')\n",
        "\n",
        "noise_311_data = pd.read_feather('data/noise_311_cleaned.feather')\n",
        "\n",
        "# Calculate the size of the Feather file\n",
        "feather_size_mb = os.path.getsize('data/noise_311_cleaned.feather') / 1e6\n",
        "print(f\"Feather file size: {feather_size_mb:.2f} MB\")\n",
        "\n",
        "valid_boroughs = ['QUEENS', 'BROOKLYN', 'BRONX', 'MANHATTAN', 'STATEN ISLAND']\n",
        "\n",
        "# Identify invalid borough entries\n",
        "invalid_borough_entries = noise_311_data[~noise_311_data['borough'].isin(valid_boroughs)]\n",
        "print(f\"Invalid boroughs found:{invalid_borough_entries['borough'].unique()}\")\n",
        "\n",
        "noise_311_data = noise_311_data[noise_311_data['borough'].isin(valid_boroughs)]\n",
        "\n",
        "noise_311_data = noise_311_data.dropna(subset=['incident_zip']).copy()\n",
        "\n",
        "noise_311_data['incident_zip'] = noise_311_data['incident_zip'].astype(int).astype(str)\n",
        "\n",
        "# Initialize the zip code search engine\n",
        "zip_search_engine = SearchEngine()\n",
        "\n",
        "# Function to validate zip codes\n",
        "def is_valid_zipcode(zipcode):\n",
        "    zip_info = zip_search_engine.by_zipcode(zipcode)\n",
        "    if zip_info:\n",
        "        if zip_info.zipcode is not None:\n",
        "            return True\n",
        "        else:\n",
        "            return False\n",
        "    else:\n",
        "        return False\n",
        "\n",
        "valid_zip_records = noise_311_data[noise_311_data['incident_zip'].apply(is_valid_zipcode)]\n",
        "\n",
        "invalid_zip_records = noise_311_data[~noise_311_data['incident_zip'].apply(is_valid_zipcode)]\n",
        "\n",
        "print(f\"Number of records with valid zip codes: {valid_zip_records.shape[0]}\")\n",
        "\n",
        "# Convert date columns to datetime format\n",
        "noise_311_data['created_date'] = pd.to_datetime(noise_311_data['created_date'], errors='coerce')\n",
        "noise_311_data['closed_date'] = pd.to_datetime(noise_311_data['closed_date'], errors='coerce')\n",
        "noise_311_data['resolution_action_updated_date'] = pd.to_datetime(\n",
        "    noise_311_data['resolution_action_updated_date'], errors='coerce'\n",
        ")\n",
        "\n",
        "closed_before_created = noise_311_data[noise_311_data['closed_date'] < noise_311_data['created_date']]\n",
        "print(f\"Records where 'closed_date' is before 'created_date': {closed_before_created.shape[0]}\")\n",
        "\n",
        "same_created_closed = noise_311_data[noise_311_data['created_date'] == noise_311_data['closed_date']]\n",
        "print(f\"Records where 'created_date' equals 'closed_date': {same_created_closed.shape[0]}\")\n",
        "\n",
        "midnight_noon_records = noise_311_data[\n",
        "    noise_311_data['created_date'].dt.time.isin([pd.Timestamp('00:00:00').time(), pd.Timestamp('12:00:00').time()]) |\n",
        "    noise_311_data['closed_date'].dt.time.isin([pd.Timestamp('00:00:00').time(), pd.Timestamp('12:00:00').time()])\n",
        "]\n",
        "print(f\"Records with times at exactly midnight or noon: {midnight_noon_records.shape[0]}\")\n",
        "\n",
        "action_after_closed = noise_311_data[\n",
        "    noise_311_data['resolution_action_updated_date'] > noise_311_data['closed_date']\n",
        "]\n",
        "print(f\"Records where 'resolution_action_updated_date' is after 'closed_date': {action_after_closed.shape[0]}\")\n",
        "\n",
        "print(\n",
        "    \"Variables with no entries like 'facility_type' and 'due_date' have been removed. \"\n",
        "    \"Redundant variables such as 'agency' and 'agency_name' have been eliminated. \"\n",
        "    \"Also, we checked for records with times exactly on the hour.\"\n",
        ")"
      ],
      "id": "98848056",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 2. Data Manipulation"
      ],
      "id": "f32e5898"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from datetime import time\n",
        "from plotnine import *\n",
        "from scipy import stats\n",
        "from scipy.stats import chi2_contingency\n",
        "\n",
        "# Calculate the response time in hours\n",
        "noise_311_data['response_time'] = (\n",
        "    (noise_311_data['closed_date'] - noise_311_data['created_date']).dt.total_seconds() / 3600\n",
        ")\n",
        "\n",
        "# Function to determine the time of day\n",
        "def determine_time_of_day(timestamp):\n",
        "    if time(7, 0) <= timestamp.time() < time(19, 0):\n",
        "        return 'Daytime'\n",
        "    else:\n",
        "        return 'Nighttime'\n",
        "\n",
        "# Create 'time_of_day' column\n",
        "noise_311_data['time_of_day'] = noise_311_data['created_date'].apply(determine_time_of_day)\n",
        "\n",
        "# Add 'day_of_week' column\n",
        "noise_311_data['day_of_week'] = noise_311_data['created_date'].dt.weekday\n",
        "\n",
        "# Initialize an empty list to store the weekend or weekday labels\n",
        "is_weekend_list = []\n",
        "\n",
        "# Loop through each value in the 'day_of_week' column\n",
        "for day in noise_311_data['day_of_week']:\n",
        "    if day >= 5:\n",
        "        is_weekend_list.append('Weekend')\n",
        "    else:\n",
        "        is_weekend_list.append('Weekday')\n",
        "\n",
        "# Add the 'is_weekend' column to the DataFrame\n",
        "noise_311_data['is_weekend'] = is_weekend_list\n",
        "\n",
        "print(noise_311_data.columns)\n",
        "\n",
        "# Plot 1: Histogram of response times by time of day and borough\n",
        "aes_mapping_1 = aes(x='response_time', fill='time_of_day')\n",
        "histogram_1 = geom_histogram(bins=5, position='dodge')\n",
        "labels_1 = labs(\n",
        "    title='Response Time Distribution by Time of Day',\n",
        "    x='Response Time (hours)',\n",
        "    y='Count'\n",
        ")\n",
        "theme_1 = theme(\n",
        "    axis_text_x=element_text(rotation=45),\n",
        "    legend_position='bottom'\n",
        ")\n",
        "facet_1 = facet_wrap('~ borough')\n",
        "plot_1 = ggplot(noise_311_data, aes_mapping_1) + histogram_1 + labels_1 + theme_1 + facet_1\n",
        "print(plot_1)\n",
        "\n",
        "# Plot 2: Histogram of response times by time of day and complaint type\n",
        "aes_mapping_2 = aes(x='response_time', fill='time_of_day')\n",
        "histogram_2 = geom_histogram(bins=5, position='dodge')\n",
        "labels_2 = labs(\n",
        "    title='Response Time Distribution by Time of Day',\n",
        "    x='Response Time (hours)',\n",
        "    y='Count'\n",
        ")\n",
        "theme_2 = theme(\n",
        "    axis_text_x=element_text(rotation=45),\n",
        "    legend_position='bottom'\n",
        ")\n",
        "facet_2 = facet_wrap('~ complaint_type')\n",
        "plot_2 = ggplot(noise_311_data, aes_mapping_2) + histogram_2 + labels_2 + theme_2 + facet_2\n",
        "print(plot_2)\n",
        "\n",
        "\n",
        "# Plot 3: Histogram with complaint type and borough facets\n",
        "aes_mapping_3 = aes(x='response_time', fill='time_of_day')\n",
        "histogram_3 = geom_histogram(bins=5)\n",
        "labels_3 = labs(\n",
        "    title='Response Time Distribution',\n",
        "    x='Response Time (hours)',\n",
        "    y='Count'\n",
        ")\n",
        "theme_3 = theme(\n",
        "    axis_text_x=element_text(rotation=45),\n",
        "    legend_position='bottom',\n",
        "    figure_size=(7, 9)\n",
        ")\n",
        "facet_3 = facet_grid('complaint_type ~ borough')\n",
        "plot_3 = ggplot(noise_311_data, aes_mapping_3) + histogram_3 + labels_3 + theme_3 + facet_3\n",
        "print(plot_3)\n",
        "\n",
        "# ANOVA test for boroughs\n",
        "borough_groups = [grp['response_time'].values for _, grp in noise_311_data.groupby('borough')]\n",
        "f_stat_borough, p_value_borough = stats.f_oneway(*borough_groups)\n",
        "print(f\"ANOVA test for boroughs: F-statistic = {f_stat_borough:.5f}, P-value = {p_value_borough:.5f}\")\n",
        "\n",
        "# ANOVA test for complaint types\n",
        "complaint_groups = [grp['response_time'].values for _, grp in noise_311_data.groupby('complaint_type')]\n",
        "f_stat_complaint, p_value_complaint = stats.f_oneway(*complaint_groups)\n",
        "print(f\"ANOVA test for complaint types: F-statistic = {f_stat_complaint:.5f}, P-value = {p_value_complaint:.5f}\")\n",
        "\n",
        "# ANOVA test for days of the week\n",
        "day_groups = [grp['response_time'].values for _, grp in noise_311_data.groupby('day_of_week')]\n",
        "f_stat_day, p_value_day = stats.f_oneway(*day_groups)\n",
        "print(f\"ANOVA test for days of the week: F-statistic = {f_stat_day:.5f}, P-value = {p_value_day:.5f}\")\n",
        "\n",
        "# Create a binary variable for response times over 2 hours\n",
        "noise_311_data['over_two_hours'] = (noise_311_data['response_time'] >= 2).astype(int)\n",
        "print(noise_311_data[['created_date', 'closed_date', 'response_time', 'over_two_hours']].head())\n",
        "\n",
        "# Chi-squared test for 'over_two_hours' vs 'complaint_type'\n",
        "complaint_crosstab = pd.crosstab(noise_311_data['over_two_hours'], noise_311_data['complaint_type'])\n",
        "print(complaint_crosstab)\n",
        "chi2_complaint, p_complaint, _, _ = chi2_contingency(complaint_crosstab)\n",
        "print(f\"Chi-squared test for 'over_two_hours' vs 'complaint_type': Chi2 = {chi2_complaint:.5f}, P-value = {p_complaint:.5f}\")\n",
        "\n",
        "# Chi-squared test for 'over_two_hours' vs 'borough'\n",
        "borough_crosstab = pd.crosstab(noise_311_data['over_two_hours'], noise_311_data['borough'])\n",
        "print(borough_crosstab)\n",
        "chi2_borough, p_borough, _, _ = chi2_contingency(borough_crosstab)\n",
        "print(f\"Chi-squared test for 'over_two_hours' vs 'borough': Chi2 = {chi2_borough:.5f}, P-value = {p_borough:.5f}\")\n",
        "\n",
        "# Chi-squared test for 'over_two_hours' vs 'day_of_week'\n",
        "day_crosstab = pd.crosstab(noise_311_data['over_two_hours'], noise_311_data['day_of_week'])\n",
        "print(day_crosstab)\n",
        "chi2_day, p_day, _, _ = chi2_contingency(day_crosstab)\n",
        "print(f\"Chi-squared test for 'over_two_hours' vs 'day_of_week': Chi2 = {chi2_day:.5f}, P-value = {p_day:.5f}\")"
      ],
      "id": "686be84c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 3. Data Analysis"
      ],
      "id": "facb9955"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from geopy.geocoders import Nominatim\n",
        "from time import sleep\n",
        "import numpy as np\n",
        "from geopy.distance import geodesic\n",
        "from uszipcode import SearchEngine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegressionCV\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, confusion_matrix,\n",
        "    f1_score, roc_curve, auc\n",
        ")\n",
        "\n",
        "precinct_addresses = pd.read_csv('data/nypd_precincts.csv', encoding='windows-1252')\n",
        "\n",
        "geolocator = Nominatim(user_agent=\"nypd_precincts_geocoder\")\n",
        "\n",
        "# Function to geocode addresses\n",
        "def geocode_address(address, max_attempts=3):\n",
        "    if pd.isnull(address) or address.strip() == \"\":\n",
        "        return np.nan, np.nan\n",
        "    attempts = 0\n",
        "    while attempts < max_attempts:\n",
        "        try:\n",
        "            location = geolocator.geocode(address)\n",
        "            if location:\n",
        "                return location.latitude, location.longitude\n",
        "            else:\n",
        "                return np.nan, np.nan\n",
        "        except Exception:\n",
        "            attempts += 1\n",
        "            sleep(1)\n",
        "    return np.nan, np.nan\n",
        "\n",
        "precinct_addresses['latitude'], precinct_addresses['longitude'] = zip(\n",
        "    *precinct_addresses['Address'].apply(geocode_address)\n",
        ")\n",
        "\n",
        "precinct_addresses.to_csv('data/nypd_precincts_geocoded.csv', index=False)\n",
        "\n",
        "print(precinct_addresses.head())\n",
        "print(noise_311_data.columns)\n",
        "\n",
        "geocoded_precincts = pd.read_csv('data/nypd_precincts_geocoded.csv')\n",
        "\n",
        "noise_311_data = noise_311_data.dropna(subset=['latitude', 'longitude'])\n",
        "\n",
        "def calculate_min_distance(lat, lon, precincts_df):\n",
        "    if pd.isnull(lat) or pd.isnull(lon):\n",
        "        return np.nan\n",
        "    distances = []\n",
        "    i = 0\n",
        "    num_precincts = len(precincts_df)\n",
        "    while i < num_precincts:\n",
        "        precinct = precincts_df.iloc[i]\n",
        "        if pd.notnull(precinct['latitude']) and pd.notnull(precinct['longitude']):\n",
        "            precinct_location = (precinct['latitude'], precinct['longitude'])\n",
        "            incident_location = (lat, lon)\n",
        "            distance = geodesic(incident_location, precinct_location).miles\n",
        "            distances.append(distance)\n",
        "        i += 1\n",
        "    if distances:\n",
        "        return min(distances)\n",
        "    else:\n",
        "        return np.nan\n",
        "\n",
        "noise_311_data['distance_to_precinct'] = noise_311_data.apply(\n",
        "    lambda row: calculate_min_distance(row['latitude'], row['longitude'], geocoded_precincts),\n",
        "    axis=1\n",
        ")\n",
        "\n",
        "print(noise_311_data[['latitude', 'longitude', 'distance_to_precinct']].head())\n",
        "noise_311_data['incident_zip'] = noise_311_data['incident_zip'].astype(str).str.zfill(5)\n",
        "\n",
        "valid_zip_data = noise_311_data.dropna(subset=['incident_zip']).copy()\n",
        "\n",
        "unique_zip_codes = valid_zip_data['incident_zip'].unique()\n",
        "\n",
        "zip_search_engine = SearchEngine()\n",
        "\n",
        "zip_info_list = []\n",
        "for zip_code in unique_zip_codes:\n",
        "    zip_result = zip_search_engine.by_zipcode(zip_code)\n",
        "    \n",
        "    if zip_result:\n",
        "        zipcode_type = zip_result.zipcode_type\n",
        "    else:\n",
        "        zipcode_type = None\n",
        "\n",
        "    if zip_result:\n",
        "        major_city = zip_result.major_city\n",
        "    else:\n",
        "        major_city = None\n",
        "\n",
        "    if zip_result:\n",
        "        post_office_city = zip_result.post_office_city\n",
        "    else:\n",
        "        post_office_city = None\n",
        "\n",
        "    if zip_result:\n",
        "        common_city_list = zip_result.common_city_list\n",
        "    else:\n",
        "        common_city_list = None\n",
        "\n",
        "    if zip_result:\n",
        "        county = zip_result.county\n",
        "    else:\n",
        "        county = None\n",
        "\n",
        "    if zip_result:\n",
        "        state = zip_result.state\n",
        "    else:\n",
        "        state = None\n",
        "\n",
        "    if zip_result:\n",
        "        lat = zip_result.lat\n",
        "    else:\n",
        "        lat = None\n",
        "\n",
        "    if zip_result:\n",
        "        lng = zip_result.lng\n",
        "    else:\n",
        "        lng = None\n",
        "\n",
        "    if zip_result:\n",
        "        timezone = zip_result.timezone\n",
        "    else:\n",
        "        timezone = None\n",
        "\n",
        "    if zip_result:\n",
        "        population = zip_result.population\n",
        "    else:\n",
        "        population = None\n",
        "\n",
        "    if zip_result:\n",
        "        population_density = zip_result.population_density\n",
        "    else:\n",
        "        population_density = None\n",
        "\n",
        "    if zip_result:\n",
        "        housing_units = zip_result.housing_units\n",
        "    else:\n",
        "        housing_units = None\n",
        "\n",
        "    if zip_result:\n",
        "        occupied_housing_units = zip_result.occupied_housing_units\n",
        "    else:\n",
        "        occupied_housing_units = None\n",
        "\n",
        "    if zip_result:\n",
        "        median_home_value = zip_result.median_home_value\n",
        "    else:\n",
        "        median_home_value = None\n",
        "\n",
        "    if zip_result:\n",
        "        median_household_income = zip_result.median_household_income\n",
        "    else:\n",
        "        median_household_income = None\n",
        "\n",
        "    if zip_result:\n",
        "        land_area_in_sqmi = zip_result.land_area_in_sqmi\n",
        "    else:\n",
        "        land_area_in_sqmi = None\n",
        "\n",
        "    if zip_result:\n",
        "        water_area_in_sqmi = zip_result.water_area_in_sqmi\n",
        "    else:\n",
        "        water_area_in_sqmi = None\n",
        "\n",
        "    zip_info_list.append({\n",
        "        \"incident_zip\": zip_code,\n",
        "        \"zipcode_type\": zipcode_type,\n",
        "        \"major_city\": major_city,\n",
        "        \"post_office_city\": post_office_city,\n",
        "        \"common_city_list\": common_city_list,\n",
        "        \"county\": county,\n",
        "        \"state\": state,\n",
        "        \"lat\": lat,\n",
        "        \"lng\": lng,\n",
        "        \"timezone\": timezone,\n",
        "        \"population\": population,\n",
        "        \"population_density\": population_density,\n",
        "        \"housing_units\": housing_units,\n",
        "        \"occupied_housing_units\": occupied_housing_units,\n",
        "        \"median_home_value\": median_home_value,\n",
        "        \"median_household_income\": median_household_income,\n",
        "        \"land_area_in_sqmi\": land_area_in_sqmi,\n",
        "        \"water_area_in_sqmi\": water_area_in_sqmi,\n",
        "    })\n",
        "\n",
        "zip_info_df = pd.DataFrame(zip_info_list)\n",
        "\n",
        "merged_data = pd.merge(valid_zip_data, zip_info_df, on='incident_zip', how='left')\n",
        "\n",
        "print(merged_data.head())\n",
        "\n",
        "feather.write_feather(merged_data, 'data/merged_noise_311_data.feather')\n",
        "\n",
        "merged_data = pd.read_feather('data/merged_noise_311_data.feather')\n",
        "\n",
        "merged_data_dummies = pd.get_dummies(\n",
        "    merged_data,\n",
        "    columns=['complaint_type', 'borough', 'time_of_day'],\n",
        "    drop_first=True\n",
        ")\n",
        "\n",
        "merged_data_dummies['is_weekend'] = merged_data_dummies['is_weekend'].map({'Weekday': 0, 'Weekend': 1})\n",
        "\n",
        "feature_columns = [\n",
        "    'incident_zip', 'day_of_week', 'distance_to_precinct', 'population', 'population_density',\n",
        "    'housing_units', 'occupied_housing_units', 'median_home_value', 'median_household_income', 'land_area_in_sqmi'\n",
        "] + \\\n",
        "list(merged_data_dummies.columns[merged_data_dummies.columns.str.startswith('complaint_type_')]) + \\\n",
        "list(merged_data_dummies.columns[merged_data_dummies.columns.str.startswith('borough_')]) + \\\n",
        "list(merged_data_dummies.columns[merged_data_dummies.columns.str.startswith('time_of_day_')]) + \\\n",
        "['is_weekend']\n",
        "\n",
        "X = merged_data_dummies[feature_columns]\n",
        "y = merged_data_dummies['over_two_hours']\n",
        "\n",
        "imputer = SimpleImputer(strategy='mean')\n",
        "X_imputed = imputer.fit_transform(X)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_imputed, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "reg_strength_values = np.logspace(-4, 4, 20)\n",
        "\n",
        "cv_folds = 5\n",
        "\n",
        "penalty_type = 'l1'\n",
        "\n",
        "solver_type = 'liblinear'\n",
        "\n",
        "scoring_metric = 'accuracy'\n",
        "\n",
        "max_iterations = 10000\n",
        "\n",
        "rand_state = 42\n",
        "\n",
        "logistic_cv = LogisticRegressionCV(\n",
        "    Cs=reg_strength_values,\n",
        "    cv=cv_folds,\n",
        "    penalty=penalty_type,\n",
        "    solver=solver_type,\n",
        "    scoring=scoring_metric,\n",
        "    max_iter=max_iterations,\n",
        "    random_state=rand_state\n",
        ")\n",
        "\n",
        "logistic_cv.fit(X_train, y_train)\n",
        "\n",
        "print(f\"Best C value: {logistic_cv.C_[0]}\")\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "y_pred = logistic_cv.predict(X_test)\n",
        "accuracy_test = accuracy_score(y_test, y_pred)\n",
        "print(f\"Test accuracy: {accuracy_test:.2f}\")\n",
        "\n",
        "# Extract model coefficients\n",
        "model_coefficients = logistic_cv.coef_.copy()\n",
        "model_coefficients[np.abs(model_coefficients) < 1e-4] = 0\n",
        "print(\"Model coefficients:\")\n",
        "print(model_coefficients)\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_matrix)\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "print(f\"Precision: {precision:.2f}\")\n",
        "print(f\"Recall: {recall:.2f}\")\n",
        "\n",
        "# Compute ROC curve and AUC\n",
        "y_probabilities = logistic_cv.predict_proba(X_test)[:, 1]\n",
        "false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_probabilities)\n",
        "roc_auc_value = auc(false_positive_rate, true_positive_rate)\n",
        "print(f\"AUC: {roc_auc_value:.2f}\")\n",
        "\n",
        "# Compute F1 scores for different thresholds\n",
        "f1_scores = []\n",
        "for threshold in thresholds:\n",
        "    y_threshold_pred = (y_probabilities >= threshold).astype(int)\n",
        "    f1 = f1_score(y_test, y_threshold_pred)\n",
        "    f1_scores.append(f1)\n",
        "\n",
        "# Identify the best threshold\n",
        "optimal_threshold = thresholds[np.argmax(f1_scores)]\n",
        "optimal_f1_score = max(f1_scores)\n",
        "print(f\"Optimal threshold: {optimal_threshold:.4f}\")\n",
        "print(f\"Optimal F1 score: {optimal_f1_score:.2f}\")"
      ],
      "id": "4e1df801",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "C:\\Users\\aansh\\AppData\\Local\\Programs\\Python\\Python312\\share\\jupyter\\kernels\\python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}